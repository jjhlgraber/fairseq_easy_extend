{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install fairseq==0.12.2 sacremoses sacrebleu>=1.4.12 unbabel-comet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation of the objective function\n",
    "To use it, merge it with rl_criterion.py from https://github.com/afeena/fairseq_easy_extend/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra imports\n",
    "from sacrebleu import sentence_bleu, sentence_chrf\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from comet import download_model, load_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_criterion(\"rl_loss\", dataclass=RLCriterionConfig)\n",
    "class RLCriterion(FairseqCriterion):\n",
    "    def __init__(self, task, sentence_level_metric):\n",
    "        super().__init__(task)\n",
    "        self.metric = sentence_level_metric.lower()\n",
    "        if self.metric == \"bleu\":\n",
    "            self.metric_func = sentence_bleu\n",
    "        elif self.metric == \"chrf\":\n",
    "            self.metric_func = sentence_chrf\n",
    "        elif self.metric == \"comet\":\n",
    "            model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "            model = load_from_checkpoint(model_path)\n",
    "            self.metric_func = model.predict\n",
    "        else:\n",
    "            raise Exception(\"RL metric not yet implemented\")\n",
    "        self.tokenizer = encoders.build_tokenizer(Namespace(tokenizer=\"moses\"))\n",
    "        self.tgt_dict = task.target_dictionary\n",
    "        self.src_dict = task.source_dictionary\n",
    "\n",
    "    def forward(self, model, sample, reduce=True):\n",
    "        \"\"\"Compute the loss for the given sample.\n",
    "        Returns a tuple with three elements:\n",
    "        1) the loss\n",
    "        2) the sample size, which is used as the denominator for the gradient\n",
    "        3) logging outputs to display while training\n",
    "        \"\"\"\n",
    "        nsentences, ntokens = sample[\"nsentences\"], sample[\"ntokens\"]\n",
    "        # B x T\n",
    "        src_tokens, src_lengths = (\n",
    "            sample[\"net_input\"][\"src_tokens\"],\n",
    "            sample[\"net_input\"][\"src_lengths\"],\n",
    "        )\n",
    "        tgt_tokens, prev_output_tokens = sample[\"target\"], sample[\"prev_target\"]\n",
    "        outputs = model(src_tokens, src_lengths, prev_output_tokens, tgt_tokens)\n",
    "        # get loss only on tokens, not on lengths\n",
    "        outs = outputs[\"word_ins\"].get(\"out\", None)\n",
    "        # masks = outputs[\"word_ins\"].get(\"mask\", None)\n",
    "\n",
    "        loss, reward = self._compute_loss(\n",
    "            outs,\n",
    "            tgt_tokens,\n",
    "            src_tokens,\n",
    "            #   masks\n",
    "        )\n",
    "\n",
    "        # NOTE:\n",
    "        # we don't need to use sample_size as denominator for the gradient\n",
    "        # here sample_size is just used for logging\n",
    "        sample_size = 1\n",
    "        logging_output = {\n",
    "            \"loss\": loss.detach(),\n",
    "            \"nll_loss\": loss.detach(),\n",
    "            \"ntokens\": ntokens,\n",
    "            \"nsentences\": nsentences,\n",
    "            \"sample_size\": sample_size,\n",
    "            \"reward\": reward.detach(),\n",
    "        }\n",
    "\n",
    "        return loss, sample_size, logging_output\n",
    "\n",
    "    def decode(self, toks, escape_unk=False, dict=\"tgt\"):\n",
    "        with torch.no_grad():\n",
    "            if dict == \"tgt\":\n",
    "                s = self.tgt_dict.string(\n",
    "                    toks.int().cpu(),\n",
    "                    \"@@ \",\n",
    "                    # The default unknown string in fairseq is `<unk>`, but\n",
    "                    # this is tokenized by sacrebleu as `< unk >`, inflating\n",
    "                    # BLEU scores. Instead, we use a somewhat more verbose\n",
    "                    # alternative that is unlikely to appear in the real\n",
    "                    # reference, but doesn't get split into multiple tokens.\n",
    "                    unk_string=(\n",
    "                        \"UNKNOWNTOKENINREF\" if escape_unk else \"UNKNOWNTOKENINHYP\"\n",
    "                    ),\n",
    "                )\n",
    "            else:\n",
    "                s = self.src_dict.string(\n",
    "                    toks.int().cpu(),\n",
    "                    \"@@ \",\n",
    "                    unk_string=(\n",
    "                        \"UNKNOWNTOKENINREF\" if escape_unk else \"UNKNOWNTOKENINHYP\"\n",
    "                    ),\n",
    "                )\n",
    "            s = self.tokenizer.decode(s)\n",
    "        return s\n",
    "\n",
    "    def compute_reward(self, sample_idx, targets, src_tokens):\n",
    "        \"\"\" \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if self.metric == \"comet\":\n",
    "                batch = [\n",
    "                    {\n",
    "                        \"src\": self.decode(src_tokens_sent, dict=\"src\"),\n",
    "                        \"mt\": self.decode(sample_sent),\n",
    "                        \"ref\": self.decode(target),\n",
    "                    }\n",
    "                    for sample_sent, target, src_tokens_sent in zip(\n",
    "                        sample_idx, targets, src_tokens\n",
    "                    )\n",
    "                ]\n",
    "                reward = torch.tensor(\n",
    "                    self.metric_func(batch, batch_size=64, progress_bar=False).scores\n",
    "                )\n",
    "            else:\n",
    "                sampled_sentences_strings = [\n",
    "                    self.decode(sample_idx_sent) for sample_idx_sent in sample_idx\n",
    "                ]\n",
    "\n",
    "                targets_strings = [\n",
    "                    self.decode(\n",
    "                        target_sent,\n",
    "                    )\n",
    "                    for target_sent in targets\n",
    "                ]\n",
    "                reward = torch.tensor(\n",
    "                    [\n",
    "                        self.metric_func(sampled_sentence, [target]).score\n",
    "                        for sampled_sentence, target in zip(\n",
    "                            sampled_sentences_strings, targets_strings\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            return reward\n",
    "\n",
    "    def _compute_loss(self, outputs, targets, src_tokens, masks=None):\n",
    "        \"\"\"\n",
    "        outputs: batch x len x d_model\n",
    "        targets: batch x len\n",
    "        masks:   batch x len\n",
    "        \"\"\"\n",
    "        # Locate possible padding tokens for masking later\n",
    "        masks = targets.ne(self.tgt_dict.pad())\n",
    "        bsz, seq_len, vocab_size = outputs.size()\n",
    "        # Flatten for sampling\n",
    "        probs = F.softmax(outputs, dim=-1).view(-1, vocab_size)\n",
    "        # Bring back to sentence view after sampling\n",
    "        sample_idx = torch.multinomial(probs, 1, replacement=True).view(bsz, seq_len)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ####HERE calculate metric###\n",
    "            reward = self.compute_reward(sample_idx, targets, src_tokens)\n",
    "\n",
    "        # expand it to make it of a shape BxT - each token gets the same reward value (e.g. bleu is 20, so each token gets reward of 20 [20,20,20,20,20])\n",
    "        reward = reward.unsqueeze(1).repeat(1, seq_len)\n",
    "        # now you need to apply mask on both outputs and reward\n",
    "        if masks is not None:\n",
    "            outputs, targets = outputs[masks], targets[masks]\n",
    "            reward, sample_idx = reward[masks], sample_idx[masks]\n",
    "        # numerically more stable than log on probs\n",
    "        log_probs = F.log_softmax(outputs, dim=-1)\n",
    "        # select the log probs for the sampled indices\n",
    "        log_probs_of_samples = log_probs.gather(1, sample_idx.unsqueeze(1)).squeeze()\n",
    "        # compute loss\n",
    "        loss = -log_probs_of_samples * reward.to(log_probs_of_samples.device)\n",
    "        return loss.mean(), reward.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
